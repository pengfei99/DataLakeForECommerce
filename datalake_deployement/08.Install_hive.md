# 7. Hive

To understand hive and hive metastore, read this page: https://data-flair.training/blogs/apache-hive-metastore/

## 7.1 Install hive
You can download the latest stable version from this [site](https://hive.apache.org/downloads.html)
The current latest stable version is hive 3.1.2

Download the tar ball and untar it under **/opt/module/**

### 7.1.1 Configure env var for hive
Suppose the hive home path is /opt/module/hive-3.1.2

```shell

#add the hive home to the path
vim /etc/profile.d/hive.sh

# add following line to the file
export HIVE_HOME=/opt/module/hive-3.1.2
export PATH=$PATH:$HIVE_HOME/bin

# check the flume home
source hive.sh 
echo $HIVE_HOME

```

### 7.1.2 Edit bin/hive-config.sh file

If your hadoop home is not set, you need to add hadoop home in hive-config.sh

```shell
export HADOOP_HOME=/home/hdoop/hadoop-3.2.2
```

### 7.1.3 Create Hive Directories in HDFS
Create two separate directories to store data in the HDFS layer:

- The temporary, tmp directory is going to store the intermediate results of Hive processes.
- The warehouse directory is going to store the Hive related tables.


```shell
# create tmp dir and grant w to group
hdfs dfs -mkdir /tmp
hdfs dfs -chmod g+w /tmp

# create hive warehouse dir
hdfs dfs -mkdir -p /user/hive/warehouse
hdfs dfs -chmod g+w /user/hive/warehouse

# check the newly created directories
hdfs dfs -ls /
```


### 7.1.4 Configure hive-site.xml File (Optional)
Apache Hive distributions contain template configuration files by default. The template files are located within 
the Hive conf directory and outline default Hive settings.

Use the following command to locate the correct file:

```shell
cd $HIVE_HOME/conf

# Use the hive-default.xml.template to create the hive-site.xml file:
cp hive-default.xml.template hive-site.xml

# now edit the file
vim hive-site.xml

```
Find the following line, and configure where hive should write the data on hdfs or local file system.
```xml
<property>
    <name>hive.metastore.warehouse.dir</name>
    <value>/user/hive/warehouse</value>
    <description>location of default database for the warehouse</description>
  </property>

```

Note: **The hive-site.xml file controls every aspect of Hive operations**. The number of available advanced settings 
can be overwhelming and highly specific. Consult the [official Hive Configuration Documentation](https://cwiki.apache.org/confluence/display/Hive/AdminManual+Configuration#AdminManualConfiguration-HiveConfigurationVariables)
regularly when customizing Hive and Hive Metastore settings.

### 7.1.5 Initiate Database

Hive needs a database to store metadata. Hive provides a embedded Derby database for this purpose. But it's not recommended
in a production environment. Initiate the Derby database, from the Hive bin directory using the schematool command:

```shell
$HIVE_HOME/bin/schematool -dbType derby -initSchema
```

#### Handle the guava Incompatibility Error in database initialization
If the Derby database does not successfully initiate,  you might receive an error with the following content:

“Exception in thread “main” java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V”

This error indicates that there is most likely an incompatibility issue between Hadoop and Hive guava versions.

Locate the guava jar file in the Hive lib directory and hadoop lib directory:

```shell
# find guava jar in hive
ls $HIVE_HOME/lib | grep -i "guava"

# find guava jar in hadoop
ls $HADOOP_HOME/share/hadoop/hdfs/lib | grep -i "guava"
```

You can notice the version difference
```shell
rm $HIVE_HOME/lib/guava-19.0.jar
# Copy the guava file from the Hadoop lib directory to the Hive lib directory:

cp $HADOOP_HOME/share/hadoop/hdfs/lib/guava-27.0-jre.jar $HIVE_HOME/lib/
```

#### Handle the special character Error in database initialization

You may also receive the following error when doing the database initialization

```text
Exception in thread "main" java.lang.RuntimeException: com.ctc.wstx.exc.WstxParsingException: Illegal character entity: expansion character (code 0x8
 at [row,col,system-id]: [3215,96,"file:/opt/hive/hive-3.1.2/conf/hive-site.xml"]

```
You have this error, because in the default hive-site.xml file, You have a line like below. It contains **&#8;**
(a special character). You need to remove it. 
```text
<property>
    <name>hive.txn.xlock.iow</name>
    <value>true</value>
    <description>
      Ensures commands with OVERWRITE (such as INSERT OVERWRITE) acquire Exclusive locks for&#8;transactional tables.  This ensures that inserts (w/o overwrite) running concurrently
      are not hidden by the INSERT OVERWRITE.
    </description>
  </property>

```


### 7.2 launch the hive client

If you have initialized the database correctly, you should be able to run the client right now

```shell
cd $HIVE_HOME/bin
hive
```

Note the hive use the ccurrent uid to write to hdfs, if your current user does not have the rights on HDFS, you will receive 
errors

```shell
# use runuser
sudo runuser -l hadoop -c './$HIVE_HOME/bin/hive'
```

#### Handle illegal argument exception

When you run the hive client. You may encounter the following error.
```text
ava.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: ${system:java.io.tmpdir%7D/$%7Bsystem:user.name%7D
```

This because, hive use variables to generate tmp folder. If these variables are not configured, you will receive 
the above error. 

Add the following config into the **hive-site.xml**

```xml
 <property>
    <name>system:java.io.tmpdir</name>
    <value>/tmp/hive/java</value>
  </property>
  <property>
    <name>system:user.name</name>
    <value>${user.name}</value>
  </property>
```

### 7.3 metadata database

If you use derby as metadata store database, when you run hive client, hive will create a folder in the current path.

To change this, you need to go to the hive-site.xml and find

```xml
<property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:derby:;databaseName=metastore_db;create=true</value>

```
The default value **jdbc:derby:;databaseName=metastore_db;create=true** specifies that you will be using embedded derby
as your Hive metastore and the location of the metastore is metastore_db. Also the metastore will be created if it doesn't already exist.

Note that the location of the metastore (metastore_db) is a relative path. Therefore, it gets created where you launch 
Hive from. If you update this property (in your hive-site.xml) to be, say an absolute path to a location, the 
metastore will be used from that location.

Hive uses embedded derby by default to allow an out-of-the-box experience and for ease of testing. 

For a prod environment, we need a real db like MySQL or PostgreSQL. Instructions on how to do that are available [here](https://aws.amazon.com/fr/premiumsupport/knowledge-center/postgresql-hive-metastore-emr/).

Now, suppose you have installed a postgresql server
```text
user: hive
pwd: hive
db_name: hive_meta
```
### 7.3.1 Update hive-site.xml for postgresql connection

First remove the derby conf

```xml
<!-- old Hive derby connection Parameters -->
<!-- 
  <property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:derby:;databaseName=metastore_db;create=true</value>
    <description>
      JDBC connect string for a JDBC metastore.
      To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL.
      For example, jdbc:postgresql://myhost/db?ssl=true for postgres database.
    </description>
 </property>
 
  <property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>org.apache.derby.jdbc.EmbeddedDriver</value>
    <description>Driver class name for a JDBC metastore</description>
  </property>
  
  <property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>APP</value>
    <description>Username to use against metastore database</description>
  </property>

  <property>
    <name>javax.jdo.option.ConnectionPassword</name>
    <value>mine</value>
    <description>password to use against metastore database</description>
  </property>


-->

```

You need to add the postgresql connection (e.g. following line) to the hive-site.xml


```xml
<!-- Hive postgresql connection Parameters -->
<property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:postgresql://127.0.0.1:5432/hive_meta</value>
    <description>PostgreSQL JDBC driver connection URL</description>
  </property>

  <property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>org.postgresql.Driver</value>
    <description>PostgreSQL metastore driver class name</description>
  </property>

  <property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>hive</value>
    <description>the username for the DB instance</description>
  </property>

  <property>
    <name>javax.jdo.option.ConnectionPassword</name>
    <value>hive</value>
    <description>the password for the DB instance</description>
  </property>
```

### 7.3.2 Init schema in postgresql
```shell
./schematool -dbType postgres -initSchema  
```


## 7.4 Hive on spark

Hive supports many calculation engines such as: MR (default), TEZ, Spark, Impala. Here we will show how to configure 
hive to use spark as engine.  This mode is called **Hive on spark**. End users will use hive as interface and 
HQL (hive query language) as language to write business logic query.

There is another way called **Spark on hive**, Spark uses hive metastore to store the metadata of tables(dataframe, dataset)
In Spark on hive mode, end users will use SparkSQL as interface, Scala or python as language.


### 7.4.1 Hive on spark dependencies imcompatibility configuration

Before we start, please note that there is compatibility issues between hive 3.1.2 and Spark 3.*. Because the official
build of the hive 3.1.2 used spark 2.4.5 as the spark dependency. 

To use spark 3.*, you need to download the hive 3.1.2 source, change the pom file (modify spark2.4.5 to spark 3.*).
Then do the maven build yourself.

### 7.4.2 Install spark on Hive node

Download the spark binary that you have chosen(must be compatible with spark version in the hive3.1.2 build).
Then untar it and put it under /opt/module/
```shell
tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C /opt/module/
mv /opt/module/spark-3.2.0-bin-hadoop3.2 /opt/module/spark
```

#### Configure SPARK_HOME

```shell
sudo vim /etc/profile.d/spark.sh

# add the following lines
# SPARK_HOME
export SPARK_HOME=/opt/module/spark
export PATH=$PATH:$SPARK_HOME/bin

# source the file to enable env var
source /etc/profile.d/spark.sh
echo $SPARK_HOME
```

### 7.4.3 Add spark config inside hive

Edit the following file to configure spark master connection. Here we suppose we used a spark cluster in standalone mode.
You can also use Yarn mode (the master config will be **yarn**) or k8s mode (the master config will be **k8s://https://kubernetes.default.svc:443**)

To see how to deploy a spark standalone mode on a single server, please go to chapter **09.Install_spark** 

Note before you run hive, the spark cluster must be running first. 

```shell
vim /opt/module/hive/conf/spark-defaults.conf

# add the following lines

spark.master                             spark://pengfei.org:7077
spark.eventLog.enabled                   true
spark.eventLog.dir                       hdfs://pengfei.org:9000/tmp/spark-logs
spark.executor.memory                    1g
spark.driver.memory					     1g
```

### 7.4.4 Upload spark dependencies to the cluster storage

In hive on spark mode, all the hive queries are executed by the spark cluster. The Spark executor may be on any node of 
the cluster (e.g. yarn mode), the node that runs the executor may not have the spark jars dependencies. So it's better
to have a cluster storage that hosts all the spark jar, and node can access it.

Here we recommend using the spark binary build without hadoop dependencies. Because hadoop3.2 inside spark has 
already included hive2.3.7 as dependencies. If we use it, we will have conflict with our hive 3.1.2. So to avoid it, use
the build without hadoop.

```shell
tar -zxvf /opt/software/spark-3.2.0-bin-without-hadoop.tgz

hdfs dfs -mkdir /spark-jars

# make sure anyone can use it, not for prod
hdfs dfs -chmod -R 777 /spark-jars

# copy the jars
hdfs dfs -put spark-3.2.0/jars/* /spark-jars
```


### 7.4.5 Edit hive-site.xml to enable spark as engine

```xml
  
<!--Hive execution engine config, default value is MR-->
<property>
    <name>hive.execution.engine</name>
    <value>spark</value>
</property>
```

If you are using yarn mode, you need to give the spark jar location too

```xml
<!--Spark jars location（note: the port number is the port number of the hdfs name node port number ）-->
<property>
    <name>spark.yarn.jars</name>
    <value>hdfs://pengfei.org:9000/spark-jars/*</value>
</property>
```

The default timeout for hive to wait spark driver is 90000. You can make the timeout longer.

```xml
<property>
    <name>hive.spark.client.server.connect.timeout</name>
    <value>1300000ms</value>
    <description>
      Expects a time value with unit (d/day, h/hour, m/min, s/sec, ms/msec, us/usec, ns/nsec), which is msec if not specified.
      Timeout for handshake between Hive client and remote Spark driver.  Checked by both processes.
    </description>
  </property>

```
## 7.5 Hive local mode

We can run hive without hdfs and mapreduce. 


## 7.6 Debugging 

### 7.6.1 Hive Logging
Hive uses log4j for logging. By default logs are not emitted to the console by the CLI. The default logging level is 
WARN for Hive releases prior to 0.13.0. Starting with Hive 0.13.0, the default logging level is INFO.

The logs are stored in the directory /tmp/<user.name>:

**/tmp/<user.name>/hive.log**

So, if we run the ./sbin/hive with user pliu, then the log will be located at /tmp/pliu/hive.log

To configure a different log location, set hive.log.dir in $HIVE_HOME/conf/hive-log4j.properties. Make sure the directory has the sticky bit set (chmod 1777 <dir>).

hive.log.dir=<other_location>
